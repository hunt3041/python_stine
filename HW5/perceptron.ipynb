{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5\n",
    "\n",
    "In this homework you will complete 2 questions. One from chapter 14 and one perceptron machine learning problem using scikit-learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 Least-Squares Regression Analysis\n",
    "\n",
    "Use least-squares regression to fit a straight line to the following data:\n",
    "\n",
    "| $x$  | 0  | 2  | 4  | 6  | 9  | 11 | 12 | 15 | 17 | 19 |\n",
    "|--------|----|----|----|----|----|----|----|----|----|----|\n",
    "| $y$  | 5  | 6  | 7  | 6  | 9  | 8  | 8  | 10 | 12 | 12 |\n",
    "\n",
    "1. Determine the **slope** $m$ and **intercept** $b$ of the least-squares regression line.\n",
    "2. Compute the **standard error of the estimate** $S_e$.\n",
    "3. Calculate the **coefficient of determination** $R^2$.\n",
    "\n",
    "### Formulas\n",
    "\n",
    "1. **Slope $(m$)**:  \n",
    "   $$\n",
    "   m = \\frac{n \\sum (x_i y_i) - \\sum x_i \\sum y_i}{n \\sum x_i^2 - \\sum x_i^2}\n",
    "   $$\n",
    "   where $n$ is the length of $x$\n",
    "\n",
    "2. **Intercept $(b$)**:  \n",
    "   $$\n",
    "   b = \\frac{\\sum y_i - m \\sum x_i}{n}\n",
    "   $$\n",
    "\n",
    "3. **Standard Error of the Estimate $S_e$**:  \n",
    "   $$\n",
    "   S_e = \\sqrt{\\frac{\\sum (y_i - \\hat{y}_i)^2}{n - 2}}\n",
    "   $$ \n",
    "   where $\\hat{y}_i = m x_i + b$ are the predicted $y$-values.\n",
    "\n",
    "4. **Coefficient of Determination $R^2$**:  \n",
    "   $$\n",
    "   R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "   $$  \n",
    "   where $\\bar{y} = \\frac{\\sum y_i}{n} \\text{ is the mean of } y$.\n",
    "\n",
    "### Instructions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:  0.3591455273698264\n",
      "Y-intercept:  4.88811748998665\n",
      "Standard error:  0.8510968341468516\n",
      "Coefficient of determination:  0.8928849063076091\n",
      "X:  [ 0  2  4  6  9 11 12 15 17 19]\n",
      "y:  [ 5  6  7  6  9  8  8 10 12 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   4,  16,  36,  81, 121, 144, 225, 289, 361])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grade\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "X = np.array([0, 2, 4, 6, 9, 11, 12, 15, 17, 19])\n",
    "y = np.array([5, 6, 7, 6, 9, 8, 8, 10, 12, 12])\n",
    "\n",
    "# Number of data points\n",
    "n = len(X)\n",
    "\n",
    "# Summations\n",
    "sum_x_times_y = np.sum(X*y)\n",
    "sum_x = np.sum(X)\n",
    "sum_y = np.sum(y)\n",
    "sum_x_squared = np.sum(X**2)\n",
    "\n",
    "# Calculate slope (m) and intercept (b)\n",
    "m = (n*sum_x_times_y - sum_x*sum_y)/(n*sum_x_squared - sum_x**2)\n",
    "b = (sum_y - m*sum_x)/n\n",
    "\n",
    "# Predicted y values\n",
    "y_hat = m*X + b\n",
    "\n",
    "# Calculate standard error of the estimate (Se)\n",
    "se = np.sqrt(np.sum((y - y_hat)**2)/(n - 2))\n",
    "\n",
    "# Calculate coefficient of determination (R^2)\n",
    "y_bar = sum_y/n\n",
    "r_squared = 1 - (np.sum((y - y_hat)**2))/np.sum((y - y_bar)**2)\n",
    "\n",
    "# Output results\n",
    "print('Slope: ', m)\n",
    "print('Y-intercept: ', b)\n",
    "print('Standard error: ', se)\n",
    "print('Coefficient of determination: ', r_squared)\n",
    "print('X: ', X)\n",
    "print('y: ', y)\n",
    "\n",
    "X*y\n",
    "X**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questiom 2\n",
    "In this task, you will use the Iris dataset to train a perceptron model for classification. Follow the steps below to preprocess the data, train the model, and evaluate its performance.\n",
    "Steps:\n",
    "\n",
    "1. Load the Dataset:\n",
    "\n",
    "    Use the datasets module from scikit-learn to load the Iris dataset. Extract the petal length and petal width as the features (columns 2 and 3 of the dataset), and use the target labels (class labels).\n",
    "2. Split the Dataset:\n",
    "    \n",
    "    Use the train_test_split function to split the dataset into training and test sets. Use 70% of the data for training and 30% for testing, ensuring stratification to maintain class proportions. Print the counts of each class in the training and test sets.\n",
    "3. Standardize the Features:\n",
    "    \n",
    "    Standardize the training and test datasets using the StandardScaler class from scikit-learn. Ensure that the test set is scaled using the same parameters as the training set.\n",
    "4. Train the Perceptron:\n",
    "    \n",
    "    Train a perceptron classifier using the Perceptron class from scikit-learn. Set the learning rate (eta0) to 0.1 and random_state to 1 for reproducibility.\n",
    "5. Evaluate the Model:\n",
    "    \n",
    "    Predict the class labels for the test data using the trained perceptron. Count the number of misclassified examples and compute the accuracy using the accuracy_score function from scikit-learn.\n",
    "\n",
    "### Expected Output:\n",
    "\n",
    "| **Name**        | **Description**                                      | **Type**                                              |\n",
    "|------------------|------------------------------------------------------|-------------------------------------------------------|\n",
    "| `X`             | Input data for the model                             | `numpy.ndarray`                                       |\n",
    "| `y`             | Class labels for the input data                      | `numpy.ndarray`                                       |\n",
    "| `X_train`       | Training data                                         | `numpy.ndarray`                                       |\n",
    "| `X_test`        | Test data                                             | `numpy.ndarray`                                       |\n",
    "| `y_train`       | Class labels for the training data                   | `numpy.ndarray`                                       |\n",
    "| `y_test`        | Class labels for the test data                       | `numpy.ndarray`                                       |\n",
    "| `X_train_std`   | Standardized training data                           | `numpy.ndarray`                                       |\n",
    "| `X_test_std`    | Standardized test data                               | `numpy.ndarray`                                       |\n",
    "| `ppn`           | Perceptron model                                     | `<class 'sklearn.linear_model._perceptron.Perceptron'>` |\n",
    "| `y_pred`        | Predicted class labels for the test data             | `numpy.ndarray`                                       |\n",
    "| `accuracy`      | Accuracy of the model                                | `float`                                              |\n",
    "\n",
    "\n",
    "### Notes for Students:\n",
    "\n",
    "- Use the provided modules and methods from `scikit-learn`.\n",
    "- Ensure all steps are reproducible by setting `random_state=1` wherever applicable.\n",
    "- Provide code for each step along with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries and Load the Dataset\n",
    "We first need to import the necessary libraries and load the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target names:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "#grade\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# set X and y\n",
    "X = iris.get('data')[:, -2:]\n",
    "y = iris.get('target')\n",
    "\n",
    "# Display unique class labels\n",
    "print('target names: ', iris.get('target_names'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the Dataset into Training and Test Sets\n",
    "\n",
    "Next split the dataset into training and test sets, ensuring stratification for balanced class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {0: 50, 1: 50, 2: 50}\n",
      "Training set class distribution: {0: 36, 1: 32, 2: 37}\n",
      "Testing set class distribution: {0: 14, 1: 18, 2: 13}\n"
     ]
    }
   ],
   "source": [
    "#grade\n",
    "\n",
    "# Split into 70% training and 30% test datasets\n",
    "# define X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1, stratify=y)\n",
    "\n",
    "# Verify stratification\n",
    "# Calculate class distributions\n",
    "original_distribution = dict(zip(*np.unique(y, return_counts=True)))\n",
    "train_distribution = dict(zip(*np.unique(y_train, return_counts=True)))\n",
    "test_distribution = dict(zip(*np.unique(y_test, return_counts=True)))\n",
    "\n",
    "# Print distributions\n",
    "print(\"Original class distribution:\", original_distribution)\n",
    "print(\"Training set class distribution:\", train_distribution)\n",
    "print(\"Testing set class distribution:\", test_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Standardize the Features\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade\n",
    "\n",
    "# Standardize features\n",
    "# Define sc, fit, transform, and define X_train_std and X_test_std\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Perceptron\n",
    "\n",
    "We train a perceptron model using the Perceptron class from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 samples were misclassified\n"
     ]
    }
   ],
   "source": [
    "#grade\n",
    "\n",
    "# Train perceptron model\n",
    "# Define ppn, fit, and predict\n",
    "\n",
    "ppn = Perceptron(eta0=0.01, random_state=1)\n",
    "ppn.fit(X_train, y_train)\n",
    "\n",
    "# Predict class labels for the test set\n",
    "# Define y_pred\n",
    "y_pred = ppn.predict(X_test)\n",
    "\n",
    "# Count misclassified examples\n",
    "incorrect = np.sum(y_test != y_pred)\n",
    "print(f'{incorrect} samples were misclassified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the Model\n",
    "\n",
    "We evaluate the model’s accuracy using the `accuracy_score` function and the perceptron’s `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grade\n",
    "\n",
    "# Calculate accuracy using accuracy_score\n",
    "# Define accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. (For your enjoyment only)\n",
    "\n",
    "graph the decision boundaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-0e_fkdim because the default path (/tmp/cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plot decision boundaries for a classifier and display data points.\n",
    "\n",
    "    This function visualizes the decision boundaries of a classifier by creating a \n",
    "    grid of points across the feature space and coloring regions based on the \n",
    "    classifier's predictions. It also plots the training and test data points.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n_samples, 2)\n",
    "        Feature matrix containing two features for visualization.\n",
    "\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target labels corresponding to the samples in X.\n",
    "\n",
    "    classifier : object\n",
    "        A trained classifier with a `predict` method.\n",
    "\n",
    "    test_idx : array-like, optional (default=None)\n",
    "        Indices of the test samples within X. Test samples are highlighted with \n",
    "        a distinct marker.\n",
    "\n",
    "    resolution : float, optional (default=0.02)\n",
    "        The resolution of the grid used to plot the decision surface. Smaller \n",
    "        values result in finer granularity but require more computation.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Displays the decision boundary plot with training and test data points.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function assumes that `X` contains exactly two features.\n",
    "    - It uses `ListedColormap` to define distinct colors for up to three classes.\n",
    "    - Training samples are plotted with distinct markers and colors, and test \n",
    "      samples (if provided) are highlighted with yellow circles.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    plot_decision_regions(X_combined_std, y_combined, classifier=ppn, \n",
    "                          test_idx=range(105, 150))\n",
    "    \"\"\"\n",
    "    # Set up marker generator and color map\n",
    "    markers = ('s', 'x', 'o')\n",
    "    colors = ('red', 'blue', 'lightgreen')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # Plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(x1_min, x1_max, resolution),\n",
    "        np.arange(x2_min, x2_max, resolution),\n",
    "    )\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # Plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(\n",
    "            x=X[y == cl, 0],\n",
    "            y=X[y == cl, 1],\n",
    "            alpha=0.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f'Class {cl}',\n",
    "            edgecolor='black',\n",
    "        )\n",
    "    \n",
    "    # Highlight test samples\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c='yellow',\n",
    "            edgecolor='black',\n",
    "            alpha=1.0,\n",
    "            linewidth=1,\n",
    "            marker='o',\n",
    "            s=100,\n",
    "            label='Test set',\n",
    "        )\n",
    "\n",
    "\n",
    "# Combine standardized training and test data for visualization\n",
    "\n",
    "# Plot the decision boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
